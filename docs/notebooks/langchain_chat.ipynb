{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "370bc2f0",
      "metadata": {},
      "source": [
        "# LangChain Chat Models: Overview\n",
        "\n",
        "üë®‚Äçüíª _**Author:**_ [Mohankumar Ramachandran](https://github.com/mohankumarelec)\n",
        "\n",
        "<span id=\"meta-description\" style=\"display: none;\">Master LangChain chat models with this comprehensive guide covering initialization, execution, structured outputs, tool calling, and multimodal inputs with practical examples.</span>\n",
        "<span id=\"meta-tags\" style=\"display: none;\">langchain, langchain-openai</span>\n",
        "<span id=\"meta-created-at\" style=\"display: none;\">2025-07-12T00:00:00Z</span>\n",
        "\n",
        "Welcome to your comprehensive guide to LangChain Chat Models! üöÄ\n",
        "\n",
        "This hands-on notebook will help you understand basics of LangChain chat models, including:\n",
        "\n",
        "- ‚ö° **Initialize** chat models (various methods & providers)\n",
        "- üèÉ **Run & Stream** outputs\n",
        "- üí° **Track tokens** for smart cost control\n",
        "- üì¶ **Structured outputs** with Pydantic, TypedDict, or JSON\n",
        "- üõ†Ô∏è **Tool calling** (let AI use your functions)\n",
        "- üñºÔ∏è **Multimodal**: Mix text & images\n",
        "\n",
        "Each section includes ready-to-run examples with OpenAI, but the concepts apply to all LangChain supported providers.\n",
        "\n",
        "Ready to unlock the full power of LangChain chat models? Let's dive in! üéâ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36c10eb2",
      "metadata": {},
      "source": [
        "## Setup Model API key\n",
        "\n",
        "Run the next cell to setup a free OpenAI API key using your GitHub account, or use your own OpenAI API key.\n",
        "\n",
        "> Note: May not work for enterprise GitHub accounts due to company-specific policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a540037a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# NOTE: Set support_with_star to False if you do not want to support our work yet.\n",
        "from foreverpython import init_github_models\n",
        "\n",
        "# Run below if you already have your own OpenAI API key\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "\n",
        "# else, run below to setup a free GitHub Models API key using your GitHub account.\n",
        "await init_github_models(use_browser_cache=True, support_with_star=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047323f8-09a2-4f81-85df-f24957c6cece",
      "metadata": {},
      "source": [
        "## Install packages\n",
        "\n",
        "First, let's install the official Langchain and Langchain OpenAI Python package so you can access all the latest features and models! üì¶‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f67164-2949-4aab-a51d-c2d009d7016b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Install the Langchain and Langchain OpenAI libraries\n",
        "%pip install langchain==0.3.26 langchain-openai==0.3.27"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eabf0819",
      "metadata": {},
      "source": [
        "## Initialize Chat Models\n",
        "\n",
        "Let's start with the basics: initializing chat models. LangChain makes it easy to work with various chat models, whether you're using OpenAI, Hugging Face, or any other provider by providing a standardized interface for initialization and execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172ee1b4",
      "metadata": {},
      "source": [
        "### ü•á Method 1: Universal Setup with `init_chat_model()`\n",
        "\n",
        "Best for most cases! Simple and works with any provider.\n",
        "\n",
        "**When to use:** If you want a quick, consistent way to start any chat model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce11c5b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules from LangChain\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Send a simple message to the model\n",
        "response = chat_model.invoke(\"How are you doing?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faab935f",
      "metadata": {},
      "source": [
        "### ü•à Method 2: Provider-Specific Classes\n",
        "\n",
        "Use classes like `ChatOpenAI` if you want settings unique to that provider.\n",
        "\n",
        "**When to use:** If you need a feature only supported by a specific provider (e.g., OpenAI-only settings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adbea04",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import the ChatOpenAI class from langchain_openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize Chat OpenAI with specific parameters\n",
        "chat_model = ChatOpenAI(model=\"openai/gpt-4.1-mini\")\n",
        "\n",
        "# Send a message using the provider-specific model\n",
        "response = chat_model.invoke(\"How are you doing?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a63972e",
      "metadata": {},
      "source": [
        "### ü•â Method 3: Configurable Models\n",
        "\n",
        "Make a model that lets you pick the provider each time you use it! Great for apps that support many model vendors.\n",
        "\n",
        "**When to use:** If you want to switch models/providers dynamically in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37e60cc",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules from LangChain\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Create a configurable model without specifying model/provider upfront\n",
        "configurable_model = init_chat_model()\n",
        "\n",
        "# Invoke with runtime configuration\n",
        "response = configurable_model.invoke(\n",
        "    \"How are you doing?\",\n",
        "    config={\n",
        "        \"configurable\": {\n",
        "            \"model_provider\": \"openai\",\n",
        "            \"model\": \"openai/gpt-4.1-mini\",\n",
        "        }\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print the response content\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53552ceb",
      "metadata": {},
      "source": [
        "## Execute Chat Models\n",
        "\n",
        "Now that you know how to initialize chat models, let's explore the different ways to execute them and get responses. Each method serves different use cases:\n",
        "\n",
        "1. **Invoke Method** - Perfect for single requests where you need one response üìù\n",
        "\n",
        "2. **Stream Method** - Great for real-time responses and better user experience with long outputs üåä"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5986eee8",
      "metadata": {},
      "source": [
        "### üí° Method 1: Using `invoke()`\n",
        "\n",
        "Gets the full answer in one go. Super easy!\n",
        "\n",
        "**Use this for:** Short questions, APIs, or when you don't care about partial results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0d3f99",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules from LangChain\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Send a single message and get a complete response\n",
        "single_response = chat_model.invoke(\"How are you doing?\")\n",
        "\n",
        "# Print the content of the single response\n",
        "print(single_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80eb0031",
      "metadata": {},
      "source": [
        "### üåä Method 2: Using `stream()`\n",
        "\n",
        "Get the answer bit by bit, as it's written. Feels more like chatting!\n",
        "\n",
        "**Use this for:** Long responses, user chat UIs, or when you want to display text as it comes in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74b0acb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules from LangChain\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Stream the response chunk by chunk with pipe character as a separator\n",
        "for chunk in chat_model.stream(\"How are you doing?\"):\n",
        "    print(chunk.content, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33996b61",
      "metadata": {},
      "source": [
        "## Track Token Usage\n",
        "\n",
        "When deploying LLM applications to production, monitoring token usage is crucial for managing costs effectively. LangChain makes it easy to track token consumption across different execution methods.\n",
        "\n",
        "### Why Track Tokens?\n",
        "\n",
        "- **Budget control** - Know exactly how much each request costs üí∏\n",
        "- **Optimization** - Identify opportunities to reduce token usage üìä\n",
        "- **Monitoring** - Set up alerts for unusual consumption patterns üö®"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090566b7",
      "metadata": {},
      "source": [
        "### üïπÔ∏è Scenario 1: Token Usage with `invoke()`\n",
        "\n",
        "Easiest way! Just check the `usage_metadata` after getting your response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775750cb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Token tracking with standard invoke method\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Invoke the model and get response\n",
        "response_with_usage = chat_model.invoke(\"How are you doing?\")\n",
        "\n",
        "# Display the total token usage\n",
        "print(f\"Token usage: {response_with_usage.usage_metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191da305",
      "metadata": {},
      "source": [
        "### üïπÔ∏è Scenario 2: Token Usage with `stream()`\n",
        "\n",
        "Token info is in the last chunk. Combine all chunks to get the total usage.\n",
        "\n",
        "**Use this if:** You're using streaming and want to know total token usage after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bcc81b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Token tracking with streaming\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Initialize aggregate to accumulate chunks\n",
        "aggregate_response = None\n",
        "\n",
        "# Stream with usage tracking enabled\n",
        "for chunk in chat_model.stream(\"How are you doing?\", stream_usage=True):\n",
        "    # Accumulate chunks to get total usage\n",
        "    if aggregate_response is None:\n",
        "        aggregate_response = chunk\n",
        "    else:\n",
        "        aggregate_response = aggregate_response + chunk\n",
        "\n",
        "# Display total token usage after streaming\n",
        "print(f\"Total token usage: {aggregate_response.usage_metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92696949",
      "metadata": {},
      "source": [
        "## Structured Output\n",
        "\n",
        "One of the most powerful features in LangChain is the ability to get structured, validated outputs from language models. This is essential for building reliable applications that need to process model outputs programmatically.\n",
        "\n",
        "### The Power of Structured Output\n",
        "\n",
        "- **Data extraction** - Pull specific information from unstructured text üìã\n",
        "- **Database insertion** - Create records ready for your database üíæ\n",
        "- **Type safety** - Ensure outputs match expected formats üõ°Ô∏è\n",
        "- **API integration** - Generate responses that match your API schemas üîå"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d143377",
      "metadata": {},
      "source": [
        "### ü•á Method 1: Structured Output with Pydantic\n",
        "\n",
        "Define your expected output with a Pydantic class (validates types, adds descriptions).\n",
        "\n",
        "**When to use:** You want strong validation and easy error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd363187",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Structured Output Method 1: Using Pydantic for validation\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define a Pydantic schema for jokes\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Schema for a joke with setup, punchline, and rating.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Create a structured model that returns Joke objects\n",
        "structured_joke_model = chat_model.with_structured_output(Joke)\n",
        "\n",
        "# Get a joke as a validated Pydantic object\n",
        "joke_response = structured_joke_model.invoke(\"Tell me a joke about cats\")\n",
        "\n",
        "# Display the joke in a structured format\n",
        "joke_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5d81227",
      "metadata": {},
      "source": [
        "### ü•à Method 2: Structured Output with JSON Schema\n",
        "\n",
        "Define your output with a standard JSON Schema dict.\n",
        "\n",
        "**When to use:** You're working with APIs or want to use standard JSON validation tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff447514",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Structured Output Method 2: Using JSON Schema\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Define a JSON schema for jokes\n",
        "joke_json_schema = {\n",
        "    \"title\": \"Joke\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"setup\": {\"type\": \"string\", \"description\": \"The setup of the joke\"},\n",
        "        \"punchline\": {\"type\": \"string\", \"description\": \"The punchline to the joke\"},\n",
        "        \"rating\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"setup\", \"punchline\"],  # Rating is optional\n",
        "}\n",
        "\n",
        "# Create a structured model using JSON schema\n",
        "json_structured_model = chat_model.with_structured_output(joke_json_schema)\n",
        "\n",
        "# Get a joke as a dictionary matching the schema\n",
        "joke_dict = json_structured_model.invoke(\"Tell me a joke about cats\")\n",
        "\n",
        "# Display the joke in a structured format\n",
        "joke_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bdf0b22",
      "metadata": {},
      "source": [
        "### ü•â Method 3: Structured Output with TypedDict\n",
        "\n",
        "Use Python's TypedDict for simple structured outputs (like typed dictionaries).\n",
        "\n",
        "**When to use:** You want just a typed dictionary‚Äînot full validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4287303a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Structured Output Method 3: Using TypedDict\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define a TypedDict schema for jokes\n",
        "class JokeDict(TypedDict):\n",
        "    setup: str  # The joke setup\n",
        "    punchline: str  # The joke punchline\n",
        "    rating: int  # Joke rating from 1-10\n",
        "\n",
        "\n",
        "# Create a structured model using TypedDict\n",
        "typed_structured_model = chat_model.with_structured_output(JokeDict)\n",
        "\n",
        "# Get a joke as a typed dictionary\n",
        "typed_joke = typed_structured_model.invoke(\"Tell me a joke about cats\")\n",
        "\n",
        "# Display the joke in a structured format\n",
        "typed_joke"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9bdb85b",
      "metadata": {},
      "source": [
        "## Tools / Function Calling\n",
        "\n",
        "Tool calling allows language models to interact with external functions and APIs, dramatically expanding their capabilities. This feature is essential for building agents that can perform real-world actions.\n",
        "\n",
        "### Why Use Tool Calling?\n",
        "\n",
        "- **System integration** - Connect models to your existing tools and services üîß\n",
        "- **Mathematical operations** - Let models perform accurate calculations üßÆ\n",
        "- **Data retrieval** - Fetch real-time information from databases or APIs üì°\n",
        "- **Action execution** - Enable models to perform tasks beyond text generation üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1390f013",
      "metadata": {},
      "source": [
        "### ü•á Method 1: Python Functions as Tools\n",
        "\n",
        "Just write Python functions with type hints. Easy and quick!\n",
        "\n",
        "**When to use:** Most cases‚Äîsimple math, string tools, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f398d2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define tools as simple Python functions\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers together.\n",
        "\n",
        "    Args:\n",
        "        a: First integer to add\n",
        "        b: Second integer to add\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers together.\n",
        "\n",
        "    Args:\n",
        "        a: First integer to multiply\n",
        "        b: Second integer to multiply\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Create a list of tools\n",
        "math_tools = [add, multiply]\n",
        "\n",
        "# Bind tools to the model\n",
        "model_with_math_tools = chat_model.bind_tools(math_tools)\n",
        "\n",
        "# Ask the model to use the tools\n",
        "tool_response = model_with_math_tools.invoke(\"What is 3 * 12? Also, what is 11 + 49?\")\n",
        "\n",
        "# Display the tool calls made by the model\n",
        "tool_response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf11fa2",
      "metadata": {},
      "source": [
        "### ü•à Method 2: Pydantic Models as Tools\n",
        "\n",
        "Define tools as Pydantic classes for input validation.\n",
        "\n",
        "**When to use:** You want strong input validation or need to document tool inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1318b1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define tools as Pydantic models\n",
        "class AddOperation(BaseModel):\n",
        "    \"\"\"Add two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer to add\")\n",
        "    b: int = Field(..., description=\"Second integer to add\")\n",
        "\n",
        "\n",
        "class MultiplyOperation(BaseModel):\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer to multiply\")\n",
        "    b: int = Field(..., description=\"Second integer to multiply\")\n",
        "\n",
        "\n",
        "# Create a list of Pydantic tool schemas\n",
        "pydantic_tools = [AddOperation, MultiplyOperation]\n",
        "\n",
        "# Bind tools to the model\n",
        "pydantic_model_with_tools = chat_model.bind_tools(pydantic_tools)\n",
        "\n",
        "# Use the tools\n",
        "pydantic_tool_response = pydantic_model_with_tools.invoke(\n",
        "    \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        ")\n",
        "\n",
        "# Display the tool calls made by the model\n",
        "pydantic_tool_response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8cf357",
      "metadata": {},
      "source": [
        "### ü•â Method 3: TypedDict Classes as Tools\n",
        "\n",
        "Use TypedDict for simple tool schemas (dictionary-like inputs).\n",
        "\n",
        "**When to use:** Simple cases where you want clear input keys, but don't need full validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e25e23",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define tools as TypedDict classes\n",
        "class AddTypedDict(TypedDict):\n",
        "    \"\"\"Add two integers together.\"\"\"\n",
        "\n",
        "    # Annotated format: [type, default, description]\n",
        "    a: Annotated[int, ..., \"First integer to add\"]\n",
        "    b: Annotated[int, ..., \"Second integer to add\"]\n",
        "\n",
        "\n",
        "class MultiplyTypedDict(TypedDict):\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "\n",
        "    a: Annotated[int, ..., \"First integer to multiply\"]\n",
        "    b: Annotated[int, ..., \"Second integer to multiply\"]\n",
        "\n",
        "\n",
        "# Create tool list\n",
        "typed_dict_tools = [AddTypedDict, MultiplyTypedDict]\n",
        "\n",
        "# Bind tools to model\n",
        "typed_dict_model_with_tools = chat_model.bind_tools(typed_dict_tools)\n",
        "\n",
        "# Execute with tools\n",
        "typed_dict_response = typed_dict_model_with_tools.invoke(\n",
        "    \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        ")\n",
        "\n",
        "# Display the tool calls made by the model\n",
        "typed_dict_response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075205ce",
      "metadata": {},
      "source": [
        "### üèÖ Method 4: LangChain `@tool` Decorator\n",
        "\n",
        "Use LangChain's `@tool` decorator for extra features (like docstrings and metadata).\n",
        "\n",
        "**When to use:** When you want more advanced features or documentation for your tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bb62aa",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "\n",
        "# Define tools using the @tool decorator\n",
        "@tool\n",
        "def add_numbers(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers together.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply_numbers(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Create tool list\n",
        "decorated_tools = [add_numbers, multiply_numbers]\n",
        "\n",
        "# Bind tools to model\n",
        "decorated_model_with_tools = chat_model.bind_tools(decorated_tools)\n",
        "\n",
        "# Use the decorated tools\n",
        "decorated_response = decorated_model_with_tools.invoke(\n",
        "    \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        ")\n",
        "\n",
        "# Display the tool calls made by the model\n",
        "decorated_response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5aa8075",
      "metadata": {},
      "source": [
        "## Tool Invocation\n",
        "\n",
        "Let's put it all together! We'll define a tool üõ†Ô∏è, let the model decide when to call it, and show how you can execute the tool call and return the result to the model ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd217dab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import httpx\n",
        "from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, ToolMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "from typing import cast, List\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Define the message history\n",
        "message_history: List[AnyMessage] = [\n",
        "    HumanMessage(\"What is the current temperature in New York City?\")\n",
        "]\n",
        "\n",
        "\n",
        "# Define a tool to call the Open-Meteo weather API\n",
        "def get_weather(latitude: str, longitude: str):\n",
        "    \"\"\"Calls Open-Meteo weather API to fetch the current temperature for given coordinates\"\"\"\n",
        "    with httpx.Client() as client:\n",
        "        response = client.get(\n",
        "            \"https://api.open-meteo.com/v1/forecast\",\n",
        "            params={\n",
        "                \"longitude\": longitude,\n",
        "                \"latitude\": latitude,\n",
        "                \"current\": \"temperature_2m,wind_speed_10m\",\n",
        "                \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\",\n",
        "            },\n",
        "        )\n",
        "    return str(response.json()[\"current\"][\"temperature_2m\"])\n",
        "\n",
        "\n",
        "# Bind tools to model\n",
        "model_with_tools = chat_model.bind_tools([get_weather])\n",
        "\n",
        "# Use the decorated tools\n",
        "response = cast(AIMessage, model_with_tools.invoke(message_history))\n",
        "\n",
        "# Append the response to the message history\n",
        "message_history.append(response)\n",
        "\n",
        "# Execute the tool calls made by the model in loop\n",
        "for tool_call in response.tool_calls:\n",
        "    message_history.append(\n",
        "        ToolMessage(\n",
        "            content=get_weather(**tool_call[\"args\"]), tool_call_id=tool_call[\"id\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Use the decorated tools\n",
        "final_response = model_with_tools.invoke(message_history)\n",
        "\n",
        "# Display the final response content\n",
        "final_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0749bae6",
      "metadata": {},
      "source": [
        "## Multimodal Inputs\n",
        "\n",
        "Modern language models can process more than just text! Many models now support multimodal inputs, allowing you to send images alongside text for richer interactions.\n",
        "\n",
        "### Multimodal Capabilities\n",
        "\n",
        "- **Image analysis** - Describe, analyze, or answer questions about images üì∏\n",
        "- **Content moderation** - Check images for inappropriate content üõ°Ô∏è\n",
        "- **Data extraction** - Extract text or information from images üìÑ\n",
        "- **Visual reasoning** - Solve problems that require visual understanding üß©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbbe3550",
      "metadata": {},
      "source": [
        "### ü•á Method 1: Base64-Encoded Data\n",
        "\n",
        "Best for local files or when you want to upload the file itself.\n",
        "\n",
        "**When to use:** If the file is not available online."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "994ac0d7",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Specify your local image file path here\n",
        "local_image_path = \"./sample_data/green_pathway.jpg\"\n",
        "\n",
        "# Read an image file and return its Base64 encoded string\n",
        "with open(local_image_path, \"rb\") as image_file:\n",
        "    encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Create a multimodal message with text and image\n",
        "multimodal_message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Describe the weather in this image:\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image\",\n",
        "            \"source_type\": \"base64\",\n",
        "            \"data\": encoded_image,\n",
        "            \"mime_type\": \"image/jpeg\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Send the multimodal message\n",
        "image_analysis = chat_model.invoke([multimodal_message])\n",
        "print(f\"Image analysis: {image_analysis.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dfb8f71",
      "metadata": {},
      "source": [
        "### ü•à Method 2: External file URLs\n",
        "\n",
        "Let the model fetch the file from the internet.\n",
        "\n",
        "**When to use:** If the file is already hosted online (http/https link)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1e1643",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "chat_model = init_chat_model(model=\"openai/gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Same image URL as before\n",
        "landscape_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "\n",
        "# Create a multimodal message with URL reference\n",
        "url_multimodal_message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Describe the weather in this image:\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image\",\n",
        "            \"source_type\": \"url\",\n",
        "            \"url\": landscape_url,\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Send the message with URL-based image\n",
        "url_image_analysis = chat_model.invoke([url_multimodal_message])\n",
        "print(f\"URL image analysis: {url_image_analysis.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6884f076",
      "metadata": {},
      "source": [
        "## Acknowledgements\n",
        "\n",
        "> üí° _If you found this guide helpful, consider exploring the official docs and community resources below for deeper learning and up-to-date best practices!_\n",
        "\n",
        "Content and inspiration for this guide were drawn from the following resources:\n",
        "\n",
        "- üîó [LangChain: ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai)\n",
        "- üîó [LangChain: Chat models](https://python.langchain.com/docs/how_to/#chat-models)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
